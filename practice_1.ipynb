{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "sp_conf = SparkConf() \\\n",
    "    .setAppNaame('NewProLab Practice #1') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.executor.cores', 1) \\\n",
    "    .set('spark.executor.instances', 8)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sp_conf) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем данные в память"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('npl_news.json') \\\n",
    "    .repartition(16) \\\n",
    "    .cache()\n",
    "\n",
    "df.printSchema()\n",
    "print('Загружено новостных записей: %d' % df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чистка текста\n",
    "Удаляем знаки препинания и числа, оставляем только последовательности букв английского алфавита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_col = 'short_description'\n",
    "clustering_col = 'headline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "cleaner = F.udf(lambda s: re.sub(r'[^a-z ]', '', s), StringType())\n",
    "\n",
    "df = df \\\n",
    "    .filter(F.length(clustering_col) > 0) \\\n",
    "    .withColumn(\n",
    "        clustering_col, cleaner(F.lower(F.col(clustering_col)))\n",
    "    )\n",
    "\n",
    "df.select(clustering_col) \\\n",
    "    .show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    inputCol=clustering_col,\n",
    "    outputCol=clustering_col + '_tokens'\n",
    ")\n",
    "\n",
    "rtokenizer = RegexTokenizer(\n",
    "    inputCol=clustering_col,\n",
    "    outputCol=clustering_col + '_tokens',\n",
    "    minTokenLength=1,  # minimum token length (>= 0)\n",
    "    gaps=True,         # whether regex splits on gaps (True) or matches tokens (False)\n",
    "    pattern='\\s+',     # regex pattern (Java dialect) used for tokenizing\n",
    "    toLowercase=True   # whether to convert all characters to lowercase before tokenizing\n",
    ")\n",
    "\n",
    "tokenized_df = tokenizer.transform(df).cache()\n",
    "# tokenized_df = rtokenizer.transform(df).cache()\n",
    "\n",
    "tokenized_df.select(F.col(clustering_col + '_tokens')) \\\n",
    "    .show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df \\\n",
    "    .withColumn('token', F.explode(clustering_col + '_tokens')) \\\n",
    "    .groupBy('token') \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "swr = StopWordsRemover(\n",
    "    inputCol=clustering_col + '_tokens', \n",
    "    outputCol=clustering_col + '_clean',  \n",
    "    stopWords=stopwords.words('english') + [\n",
    "        'trump'\n",
    "    ],\n",
    "    caseSensitive=False\n",
    ")\n",
    "\n",
    "clean_df = swr.transform(tokenized_df)\n",
    "\n",
    "clean_df.select(clustering_col + '_clean') \\\n",
    "    .show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df \\\n",
    "    .withColumn('token', F.explode(clustering_col + '_clean')) \\\n",
    "    .groupBy('token') \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = clean_df \\\n",
    "    .withColumn('token', F.explode(clustering_col + '_clean')) \\\n",
    "    .distinct() \\\n",
    "    .count()\n",
    "\n",
    "print('Размер словаря: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(\n",
    "        inputCol=clustering_col + '_clean',\n",
    "        outputCol='features',\n",
    "        vocabSize=vocab_size,\n",
    "        minDF=0,\n",
    "        binary=False\n",
    "    )\n",
    "\n",
    "cv_model = cv.fit(clean_df)\n",
    "\n",
    "vectorized_df = cv_model.transform(clean_df).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(\n",
    "    inputCol=clustering_col + '_clean',\n",
    "    outputCol=\"rawFeatures\",\n",
    "    numFeatures=vocab_size,\n",
    "    binary=False\n",
    ")\n",
    "\n",
    "tf_features = hashingTF.transform(clean_df)\n",
    "\n",
    "idf = IDF(\n",
    "    inputCol=\"rawFeatures\",\n",
    "    outputCol=\"features\",\n",
    "    minDocFreq=0\n",
    ")\n",
    "\n",
    "idfModel = idf.fit(tf_features)\n",
    "\n",
    "vectorized_df = idfModel \\\n",
    "    .transform(tf_features).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "print(Word2Vec().explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    inputCol=clustering_col + '_clean',\n",
    "    outputCol='features',\n",
    "    vectorSize=32,\n",
    "    numPartitions=4,\n",
    "    minCount=1,\n",
    "    stepSize=0.025,\n",
    "    maxIter=5,\n",
    "    windowSize=4,\n",
    "    maxSentenceLength=36\n",
    ")\n",
    "\n",
    "w2v_model = w2v.fit(clean_df)\n",
    "vectorized_df = w2v_model.transform(clean_df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.findSynonyms('president', 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Смротрим результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorized_df.select('features') \\\n",
    "    .show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(\n",
    "    featuresCol='features',\n",
    "    predictionCol='prediction',\n",
    "    k=31,\n",
    "    initMode='k-means||',\n",
    "    initSteps=2,\n",
    "    tol=0.0001,\n",
    "    maxIter=20\n",
    ")\n",
    "\n",
    "kmeans_model = kmeans.fit(vectorized_df)\n",
    "summary = kmeans_model.summary\n",
    "\n",
    "result = kmeans_model.transform(vectorized_df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# размеры кластеров\n",
    "_ = [print('в кластер', x[0], 'попало', x[1], 'новостей') for x in zip(\n",
    "    map(lambda x: x.prediction, summary.cluster.collect()), summary.clusterSizes\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"prediction\")\n",
    "silhouette = evaluator.evaluate(result)\n",
    "\n",
    "print('silhouette: %.4f' % silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.groupBy('prediction', 'category') \\\n",
    "    .count() \\\n",
    "    .groupBy('prediction') \\\n",
    "    .agg(\n",
    "        F.sort_array(\n",
    "            F.collect_list(\n",
    "                F.struct(\n",
    "                    F.col('count'),\n",
    "                    F.col('category')\n",
    "                )\n",
    "            ), asc=False\n",
    "        ).alias('res')\n",
    "    ).orderBy(F.size('res')) \\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
